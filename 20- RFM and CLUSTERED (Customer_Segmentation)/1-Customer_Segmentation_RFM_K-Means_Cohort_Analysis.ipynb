{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2v-2JDuiqa0"
   },
   "source": [
    "# WELCOME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLS5y2Jqiqa1"
   },
   "source": [
    "Welcome to \"RFM Customer Segmentation & Cohort Analysis Project\". This is the first project of the Capstone Project Series, which consists of 4 different project that contain different scenarios.\n",
    "\n",
    "This is a project which you will learn what is RFM? And how to apply RFM Analysis and Customer Segmentation using K-Means Clustering. Also you will improve your Data Cleaning, Data Visualization and Exploratory Data Analysis capabilities. On the other hand you will create Cohort and Conduct Cohort Analysis. \n",
    "\n",
    "Before diving into the project, please take a look at the determines and project structure.\n",
    "\n",
    "- **NOTE:** This tutorial assumes that you already know the basics of coding in Python and are familiar with the theory behind K-Means Clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SCOFEfqiqa1"
   },
   "source": [
    "## Determines\n",
    "\n",
    "Using the [Online Retail dataset](https://archive.ics.uci.edu/ml/datasets/Online+Retail) from the UCI Machine Learning Repository for exploratory data analysis, ***Customer Segmentation***, ***RFM Analysis***, ***K-Means Clustering*** and ***Cohort Analysis***.\n",
    "\n",
    "This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n",
    "\n",
    "Feature Information:\n",
    "\n",
    "**InvoiceNo**: Invoice number. *Nominal*, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. \n",
    "<br>\n",
    "**StockCode**: Product (item) code. *Nominal*, a 5-digit integral number uniquely assigned to each distinct product.\n",
    "<br>\n",
    "**Description**: Product (item) name. *Nominal*. \n",
    "<br>\n",
    "**Quantity**: The quantities of each product (item) per transaction. *Numeric*.\n",
    "<br>\n",
    "**InvoiceDate**: Invoice Date and time. *Numeric*, the day and time when each transaction was generated.\n",
    "<br>\n",
    "**UnitPrice**: Unit price. *Numeric*, Product price per unit in sterling.\n",
    "<br>\n",
    "**CustomerID**: Customer number. *Nominal*, a 5-digit integral number uniquely assigned to each customer.\n",
    "<br>\n",
    "**Country**: Country name. *Nominal*, the name of the country where each customer resides.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "First of all, to observe the structure of the data and missing values, you can use exploratory data analysis and data visualization techniques.\n",
    "\n",
    "You must do descriptive analysis. Because you must understand the relationship of the features to each other and clear the noise and missing values in the data. After that, the data set will be ready for RFM analysis.\n",
    "\n",
    "Before starting the RFM Analysis, you will be asked to do some analysis regarding the distribution of *Orders*, *Customers* and *Countries*. These analyzes will help the company develop its sales policies and contribute to the correct use of resources.\n",
    "\n",
    "You will notice that the UK not only has the most sales revenue, but also the most customers. So you will continue to analyze only UK transactions in the next RFM Analysis, Customer Segmentation and K-Means Clustering topics.\n",
    "\n",
    "Next, you will begin RFM Analysis, a customer segmentation technique based on customers' past purchasing behavior. \n",
    "\n",
    "By using RFM Analysis, you can enable companies to develop different approaches to different customer segments so that they can get to know their customers better, observe trends better, and increase customer retention and sales revenues.\n",
    "\n",
    "You will calculate the Recency, Frequency and Monetary values of the customers in the RFM Analysis you will make using the data consisting of UK transactions. Ultimately, you have to create an RFM table containing these values.\n",
    "\n",
    "In the Customer Segmentation section, you will create an RFM Segmentation Table where you segment your customers by using the RFM table. For example, you can label the best customer as \"Big Spenders\" and the lost customer as \"Lost Customer\".\n",
    "\n",
    "We will segment the customers ourselves based on their recency, frequency, and monetary values. But can an **unsupervised learning** model do this better for us? You will use the K-Means algorithm to find the answer to this question. Then you will compare the classification made by the algorithm with the classification you have made yourself.\n",
    "\n",
    "Before applying K-Means Clustering, you should do data pre-processing. In this context, it will be useful to examine feature correlations and distributions. In addition, the data you apply for K-Means should be normalized.\n",
    "\n",
    "On the other hand, you should inform the K-means algorithm about the number of clusters it will predict. You will also try the *** Elbow method *** and *** Silhouette Analysis *** to find the optimum number of clusters.\n",
    "\n",
    "After the above operations, you will have made cluster estimation with K-Means. You should visualize the cluster distribution by using a scatter plot. You can observe the properties of the resulting clusters with the help of the boxplot. Thus you will be able to tag clusters and interpret results.\n",
    "\n",
    "Finally, you will do Cohort Analysis with the data you used at the beginning, regardless of the analysis you have done before. Cohort analysis is a subset of behavioral analytics that takes the user data and breaks them into related groups for analysis. This analysis can further be used to do customer segmentation and track metrics like retention, churn, and lifetime value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQ62QseViqa2"
   },
   "source": [
    "## Project Structures\n",
    "\n",
    "- Data Cleaning & Exploratory Data Analysis\n",
    "- RFM Analysis\n",
    "- Customer Segmentation\n",
    "- Applying K-Means Clustering\n",
    "- Create Cohort and Conduct Cohort Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsPQ1tUwiqa2"
   },
   "source": [
    "## Tasks\n",
    "\n",
    "### 1. Data Cleaning & Exploratory Data Analysis\n",
    "\n",
    "- Import Modules, Load Data & Data Review\n",
    "- Follow the Steps Below\n",
    "\n",
    "    *i. Take a look at relationships between InvoiceNo, Quantity and UnitPrice columns.*\n",
    "    \n",
    "    *ii. What does the letter \"C\" in the invoiceno column mean?*\n",
    "    \n",
    "    *iii. Handling Missing Values*\n",
    "    \n",
    "    *iv. Clean the Data from the Noise and Missing Values*\n",
    "    \n",
    "    *v. Explore the Orders*\n",
    "    \n",
    "    *vi. Explore Customers by Country*\n",
    "    \n",
    "    *vii. Explore the UK Market*\n",
    "    \n",
    "### 2. RFM Analysis\n",
    "\n",
    "- Follow the steps below\n",
    "\n",
    "   *i. Import Libraries*\n",
    "   \n",
    "   *ii. Review \"df_uk\" DataFrame (the df_uk what you create at the end of the Task 1)*\n",
    "   \n",
    "   *iii. Calculate Recency*\n",
    "   \n",
    "   *iv. Calculate Frequency*\n",
    "   \n",
    "   *v. Calculate Monetary Values*\n",
    "   \n",
    "   *vi. Create RFM Table*\n",
    "\n",
    "### 3. Customer Segmentation with RFM Scores\n",
    "- Calculate RFM Scoring\n",
    "\n",
    "    *i. Creating the RFM Segmentation Table*\n",
    " \n",
    "- Plot RFM Segments\n",
    "\n",
    "### 4. Applying K-Means Clustering\n",
    "- Data Pre-Processing and Exploring\n",
    "\n",
    "    *i. Define and Plot Feature Correlations*\n",
    " \n",
    "    *ii. Visualize Feature Distributions*\n",
    " \n",
    "    *iii. Data Normalization*\n",
    "\n",
    "- K-Means Implementation\n",
    "\n",
    "    *i. Define Optimal Cluster Number (K) by using \"Elbow Method\" and \"Silhouette Analysis\"*\n",
    " \n",
    "    *ii. Visualize the Clusters*\n",
    " \n",
    "    *iii. Assign the label*\n",
    " \n",
    "    *iv. Conclusion*\n",
    " \n",
    "### 5. Create Cohort and Conduct Cohort Analysis\n",
    "- Future Engineering\n",
    "\n",
    "    *i. Extract the Month of the Purchase*\n",
    " \n",
    "    *ii. Calculating time offset in Months i.e. Cohort Index*\n",
    " \n",
    "- Create 1st Cohort: User Number & Retention Rate \n",
    "\n",
    "    *i. Pivot Cohort and Cohort Retention*\n",
    " \n",
    "    *ii. Visualize analysis of cohort 1 using seaborn and matplotlib*\n",
    "\n",
    "- Create 2nd Cohort: Average Quantity Sold \n",
    "\n",
    "    *i. Pivot Cohort and Cohort Retention*\n",
    " \n",
    "    *ii. Visualize analysis of cohort 2 using seaborn and matplotlib*\n",
    "\n",
    "- Create 3rd Cohort: Average Sales\n",
    "\n",
    "    *i. Pivot Cohort and Cohort Retention*\n",
    " \n",
    "    *ii. Visualize analysis of cohort 3 using seaborn and matplotlib*\n",
    "    \n",
    "- **Note: There may be sub-tasks associated with each task, you will see them in order during the course of the work.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English and Turkish resources that may be useful to read before starting:\n",
    "- https://medium.com/machine-learning-t%C3%BCrkiye/crm-analizi-rfm-analizi-ve-cltv-m%C3%BC%C5%9Fteri-ya%C5%9Fam-boyu-de%C4%9Feri-36e5c3a232b1\n",
    "- https://clevertap.com/blog/rfm-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-NlVU1UQGVA"
   },
   "source": [
    "# 1. Data Cleaning & Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L63G_-Dqiqa3"
   },
   "source": [
    "## Import Modules, Load Data & Data Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install missingno\n",
    "#!pip install pyforest\n",
    "#!pip install cufflinks\n",
    "#!pip install termcolor\n",
    "##!pip install wordcloud\n",
    "#!pip install squarify\n",
    "#!pip install pyclustertend\n",
    "#!pip install yellowbrick --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "J-Zb5JfOiqa3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.6.3.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1-Import Libraies\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import missingno as msno \n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PowerTransformer \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Importing plotly and cufflinks in offline mode\n",
    "import cufflinks as cf\n",
    "import plotly.offline\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn(\"this will not show\")\n",
    "\n",
    "# Figure&Display options\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "pd.set_option('max_colwidth',200)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# makes strings colored\n",
    "import colorama\n",
    "from colorama import Fore, Style\n",
    "from termcolor import colored\n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# White grid plots for dark mode users\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV consumes less memory than Excel. --> CSV is generally faster and less complicated when compared to Excel.\n",
    "#df = pd.read_excel(\"Online Retail.xlsx\")\n",
    "#df.to_csv('Online Retail.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>536365.0</th>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6.000</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>2.550</td>\n",
       "      <td>17850.000</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536365.0</th>\n",
       "      <td>71053.000</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6.000</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.390</td>\n",
       "      <td>17850.000</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536365.0</th>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8.000</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>2.750</td>\n",
       "      <td>17850.000</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536365.0</th>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6.000</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.390</td>\n",
       "      <td>17850.000</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536365.0</th>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6.000</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.390</td>\n",
       "      <td>17850.000</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          StockCode                          Description  Quantity  \\\n",
       "InvoiceNo                                                            \n",
       "536365.0     85123A   WHITE HANGING HEART T-LIGHT HOLDER     6.000   \n",
       "536365.0  71053.000                  WHITE METAL LANTERN     6.000   \n",
       "536365.0     84406B       CREAM CUPID HEARTS COAT HANGER     8.000   \n",
       "536365.0     84029G  KNITTED UNION FLAG HOT WATER BOTTLE     6.000   \n",
       "536365.0     84029E       RED WOOLLY HOTTIE WHITE HEART.     6.000   \n",
       "\n",
       "                  InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "InvoiceNo                                                             \n",
       "536365.0  2010-12-01 08:26:00      2.550   17850.000  United Kingdom  \n",
       "536365.0  2010-12-01 08:26:00      3.390   17850.000  United Kingdom  \n",
       "536365.0  2010-12-01 08:26:00      2.750   17850.000  United Kingdom  \n",
       "536365.0  2010-12-01 08:26:00      3.390   17850.000  United Kingdom  \n",
       "536365.0  2010-12-01 08:26:00      3.390   17850.000  United Kingdom  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you don't specified \"index_col=0\", you can not drop duplicates.\n",
    "df=pd.read_excel('Online Retail.xlsx',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some Useful Functions\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def missing_values(df):\n",
    "    missing_number = df.isnull().sum().sort_values(ascending=False)\n",
    "    missing_percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n",
    "    return missing_values[missing_values['Missing_Number']>0]\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def first_looking(df):\n",
    "    print(colored(\"Shape:\", attrs=['bold']), df.shape,'\\n', \n",
    "          colored('-'*79, 'red', attrs=['bold']),\n",
    "          colored(\"\\nInfo:\\n\", attrs=['bold']), sep='')\n",
    "    print(df.info(), '\\n', \n",
    "          colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "    print(colored(\"Number of Uniques:\\n\", attrs=['bold']), df.nunique(),'\\n',\n",
    "          colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "    print(colored(\"Missing Values:\\n\", attrs=['bold']), missing_values(df),'\\n', \n",
    "          colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "    print(colored(\"All Columns:\", attrs=['bold']), list(df.columns),'\\n', \n",
    "          colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "\n",
    "    df.columns= df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n",
    "\n",
    "    print(colored(\"Columns after rename:\", attrs=['bold']), list(df.columns),'\\n',\n",
    "              colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "    \n",
    "        \n",
    "def multicolinearity_control(df):\n",
    "    feature =[]\n",
    "    collinear=[]\n",
    "    for col in df.corr().columns:\n",
    "        for i in df.corr().index:\n",
    "            if (abs(df.corr()[col][i])> .9 and abs(df.corr()[col][i]) < 1):\n",
    "                    feature.append(col)\n",
    "                    collinear.append(i)\n",
    "                    print(colored(f\"Multicolinearity alert in between:{col} - {i}\", \n",
    "                                  \"red\", attrs=['bold']), df.shape,'\\n',\n",
    "                                  colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "\n",
    "def duplicate_values(df):\n",
    "    print(colored(\"Duplicate check...\", attrs=['bold']), sep='')\n",
    "    duplicate_values = df.duplicated(subset=None, keep='first').sum()\n",
    "    if duplicate_values > 0:\n",
    "        df.drop_duplicates(keep='first', inplace=True)\n",
    "        print(duplicate_values, colored(\"Duplicates were dropped!\"),'\\n',\n",
    "              colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "    else:\n",
    "        print(colored(\"There are no duplicates\"),'\\n',\n",
    "              colored('-'*79, 'red', attrs=['bold']), sep='')     \n",
    "        \n",
    "def drop_columns(df, drop_columns):\n",
    "    if drop_columns !=[]:\n",
    "        df.drop(drop_columns, axis=1, inplace=True)\n",
    "        print(drop_columns, 'were dropped')\n",
    "    else:\n",
    "        print(colored('We will now check the missing values and if necessary will drop realted columns!', attrs=['bold']),'\\n',\n",
    "              colored('-'*79, 'red', attrs=['bold']), sep='')\n",
    "        \n",
    "def drop_null(df, limit):\n",
    "    print('Shape:', df.shape)\n",
    "    for i in df.isnull().sum().index:\n",
    "        if (df.isnull().sum()[i]/df.shape[0]*100)>limit:\n",
    "            print(df.isnull().sum()[i], 'percent of', i ,'null and were dropped')\n",
    "            df.drop(i, axis=1, inplace=True)\n",
    "            print('new shape:', df.shape)       \n",
    "    print('New shape after missing value control:', df.shape)\n",
    "    \n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mShape:\u001b[0m(541909, 7)\n",
      "\u001b[1m\u001b[31m-------------------------------------------------------------------------------\u001b[0m\u001b[1m\n",
      "Info:\n",
      "\u001b[0m\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 541909 entries, 536365.0 to 581587.0\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   StockCode    541909 non-null  object        \n",
      " 1   Description  540455 non-null  object        \n",
      " 2   Quantity     541909 non-null  float64       \n",
      " 3   InvoiceDate  541909 non-null  datetime64[ns]\n",
      " 4   UnitPrice    541909 non-null  float64       \n",
      " 5   CustomerID   406829 non-null  float64       \n",
      " 6   Country      541909 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(3), object(3)\n",
      "memory usage: 33.1+ MB\n",
      "None\n",
      "\u001b[1m\u001b[31m-------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1mNumber of Uniques:\n",
      "\u001b[0mStockCode       4070\n",
      "Description     4223\n",
      "Quantity         722\n",
      "InvoiceDate    23260\n",
      "UnitPrice       1630\n",
      "CustomerID      4372\n",
      "Country           38\n",
      "dtype: int64\n",
      "\u001b[1m\u001b[31m-------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1mMissing Values:\n",
      "\u001b[0m             Missing_Number  Missing_Percent\n",
      "CustomerID           135080            0.249\n",
      "Description            1454            0.003\n",
      "\u001b[1m\u001b[31m-------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1mAll Columns:\u001b[0m['StockCode', 'Description', 'Quantity', 'InvoiceDate', 'UnitPrice', 'CustomerID', 'Country']\n",
      "\u001b[1m\u001b[31m-------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1mColumns after rename:\u001b[0m['stockcode', 'description', 'quantity', 'invoicedate', 'unitprice', 'customerid', 'country']\n",
      "\u001b[1m\u001b[31m-------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1mDuplicate check...\u001b[0m\n",
      "5848Duplicates were dropped!\u001b[0m\n",
      "\u001b[1m\u001b[31m-------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1mWe will now check the missing values and if necessary will drop realted columns!\u001b[0m\n",
      "\u001b[1m\u001b[31m-------------------------------------------------------------------------------\u001b[0m\n",
      "Shape: (536061, 7)\n",
      "New shape after missing value control: (536061, 7)\n"
     ]
    }
   ],
   "source": [
    "first_looking(df)\n",
    "duplicate_values(df)\n",
    "drop_columns(df,[])\n",
    "drop_null(df, 90)\n",
    "# df.head()\n",
    "# df.tail()\n",
    "# df.sample(5)\n",
    "# df.describe().T\n",
    "# df.describe(include=object).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model will be created for further analysis in case of not loosing some information about customer.\n",
    "df_model = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stockcode           0\n",
       "description      1453\n",
       "quantity            0\n",
       "invoicedate         0\n",
       "unitprice           0\n",
       "customerid     134497\n",
       "country             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(401564, 7)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will drop NanN values of only \"customerid\" column.\n",
    "df = df.dropna(subset=['customerid'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Orders which do NOT have customer ID's were not made by the customers already in the dataset because the customers who in fact made some purchases already have ID's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YECMxCzUQGV7"
   },
   "source": [
    "### i. Take a look at relationships between InvoiceNo, Quantity and UnitPrice columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "nQ8vAKbbiqa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8872, 7)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['quantity'] < 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-80995.0, 80995.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"quantity\"].min(), df[\"quantity\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 7)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['unitprice'] < 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 38970.0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"unitprice\"].min(), df[\"unitprice\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "ZoHopX7-iqa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 7)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We did not see any negative value in \"unitprice\" but we have \"zero\" values. \n",
    "# These are not cancelled ones, because their ID's are not starting with \"C\" letter.\n",
    "# High probability these are gifts.\n",
    "df[(df.unitprice == 0)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiSW67N_QGV8"
   },
   "source": [
    "- We see that there are negative values in the Quantity and UnitPrice columns. These are possibly canceled and returned orders. Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_851a5_row0_col0, #T_851a5_row0_col1, #T_851a5_row0_col2, #T_851a5_row0_col5, #T_851a5_row1_col0 {\n",
       "  background-color: #440154;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_851a5_row1_col1, #T_851a5_row1_col2, #T_851a5_row1_col5 {\n",
       "  background-color: #fde725;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_851a5_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >count</th>\n",
       "      <th class=\"col_heading level0 col1\" >mean</th>\n",
       "      <th class=\"col_heading level0 col2\" >std</th>\n",
       "      <th class=\"col_heading level0 col3\" >min</th>\n",
       "      <th class=\"col_heading level0 col4\" >25%</th>\n",
       "      <th class=\"col_heading level0 col5\" >50%</th>\n",
       "      <th class=\"col_heading level0 col6\" >75%</th>\n",
       "      <th class=\"col_heading level0 col7\" >max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_851a5_level0_row0\" class=\"row_heading level0 row0\" >unitprice</th>\n",
       "      <td id=\"T_851a5_row0_col0\" class=\"data row0 col0\" >401564.000000</td>\n",
       "      <td id=\"T_851a5_row0_col1\" class=\"data row0 col1\" >3.474075</td>\n",
       "      <td id=\"T_851a5_row0_col2\" class=\"data row0 col2\" >69.767499</td>\n",
       "      <td id=\"T_851a5_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "      <td id=\"T_851a5_row0_col4\" class=\"data row0 col4\" >1.250000</td>\n",
       "      <td id=\"T_851a5_row0_col5\" class=\"data row0 col5\" >1.950000</td>\n",
       "      <td id=\"T_851a5_row0_col6\" class=\"data row0 col6\" >3.750000</td>\n",
       "      <td id=\"T_851a5_row0_col7\" class=\"data row0 col7\" >38970.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_851a5_level0_row1\" class=\"row_heading level0 row1\" >quantity</th>\n",
       "      <td id=\"T_851a5_row1_col0\" class=\"data row1 col0\" >401564.000000</td>\n",
       "      <td id=\"T_851a5_row1_col1\" class=\"data row1 col1\" >12.183293</td>\n",
       "      <td id=\"T_851a5_row1_col2\" class=\"data row1 col2\" >250.295275</td>\n",
       "      <td id=\"T_851a5_row1_col3\" class=\"data row1 col3\" >-80995.000000</td>\n",
       "      <td id=\"T_851a5_row1_col4\" class=\"data row1 col4\" >2.000000</td>\n",
       "      <td id=\"T_851a5_row1_col5\" class=\"data row1 col5\" >5.000000</td>\n",
       "      <td id=\"T_851a5_row1_col6\" class=\"data row1 col6\" >12.000000</td>\n",
       "      <td id=\"T_851a5_row1_col7\" class=\"data row1 col7\" >80995.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x25078672130>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['unitprice', 'quantity']].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stockcode</th>\n",
       "      <th>description</th>\n",
       "      <th>quantity</th>\n",
       "      <th>invoicedate</th>\n",
       "      <th>unitprice</th>\n",
       "      <th>customerid</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>536365.0</th>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6.000</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>2.550</td>\n",
       "      <td>17850.000</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536365.0</th>\n",
       "      <td>71053.000</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6.000</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.390</td>\n",
       "      <td>17850.000</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536365.0</th>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8.000</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>2.750</td>\n",
       "      <td>17850.000</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536365.0</th>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6.000</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.390</td>\n",
       "      <td>17850.000</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536365.0</th>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6.000</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.390</td>\n",
       "      <td>17850.000</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          stockcode                          description  quantity  \\\n",
       "InvoiceNo                                                            \n",
       "536365.0     85123A   WHITE HANGING HEART T-LIGHT HOLDER     6.000   \n",
       "536365.0  71053.000                  WHITE METAL LANTERN     6.000   \n",
       "536365.0     84406B       CREAM CUPID HEARTS COAT HANGER     8.000   \n",
       "536365.0     84029G  KNITTED UNION FLAG HOT WATER BOTTLE     6.000   \n",
       "536365.0     84029E       RED WOOLLY HOTTIE WHITE HEART.     6.000   \n",
       "\n",
       "                  invoicedate  unitprice  customerid         country  \n",
       "InvoiceNo                                                             \n",
       "536365.0  2010-12-01 08:26:00      2.550   17850.000  United Kingdom  \n",
       "536365.0  2010-12-01 08:26:00      3.390   17850.000  United Kingdom  \n",
       "536365.0  2010-12-01 08:26:00      2.750   17850.000  United Kingdom  \n",
       "536365.0  2010-12-01 08:26:00      3.390   17850.000  United Kingdom  \n",
       "536365.0  2010-12-01 08:26:00      3.390   17850.000  United Kingdom  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OoPE-QLiqa4"
   },
   "source": [
    "### ii. What does the letter \"C\" in the InvoiceNo column mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "PgN0C80Giqa5"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'invoiceno'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'invoiceno'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10680/1387856760.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'invoiceno'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'invoiceno'"
     ]
    }
   ],
   "source": [
    "df[df['invoiceno'].astype(str).str.contains('C')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kt32SZgJQGV8"
   },
   "source": [
    "- If the invoice number starts with the letter \"C\", it means the order was cancelled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'invoiceno'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'invoiceno'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10680/3138436597.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"invoiceno\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'invoiceno'"
     ]
    }
   ],
   "source": [
    "df[\"invoiceno\"].str.startswith('C').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'invoiceno'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'invoiceno'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10680/103269859.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"invoiceno\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'invoiceno'"
     ]
    }
   ],
   "source": [
    "df[\"invoiceno\"].str.startswith('C').value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'invoiceno'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'invoiceno'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10680/405410823.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"invoiceno\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"invoiceno\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"quantity\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"unitprice\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'invoiceno'"
     ]
    }
   ],
   "source": [
    "df[df[\"invoiceno\"].str.startswith('C')] [[\"invoiceno\",\"quantity\", \"unitprice\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'invoiceno'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'invoiceno'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10680/3093810492.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"invoiceno\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"invoiceno\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"quantity\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"unitprice\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'invoiceno'"
     ]
    }
   ],
   "source": [
    "df[df[\"invoiceno\"].str.startswith('C')] [[\"invoiceno\",\"quantity\", \"unitprice\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's do some analysis between cancelled/non-cancelled orders and quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'invoiceno'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'invoiceno'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10680/1813357282.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cancelled'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'invoiceno'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'invoiceno'"
     ]
    }
   ],
   "source": [
    "df['cancelled'] = df['invoiceno'].str.contains('C')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cancelled'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cancelled'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10680/3557457467.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cancelled'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cancelled'"
     ]
    }
   ],
   "source": [
    "df['cancelled'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replace the True's to 1 and False's to 0.\n",
    "# df['cancelled'] = df[\"cancelled\"].replace('True',1).replace('False',0) --> This is not working\n",
    "df['cancelled'] = df['cancelled']*1\n",
    "df['cancelled'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- canceled orders by Quantity > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxrhRFHhiqa5"
   },
   "outputs": [],
   "source": [
    "df[(df.cancelled == 1) & (df.quantity > 0)]   # returns us nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- non-canceled orders by Quantity < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.cancelled == 0) & (df.quantity < 0)]   # returns us nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YP5RPZq1QGV_"
   },
   "source": [
    "- When we filter canceled orders by Quantity > 0, or non-cancelled orders by Quantity <0 we get empty output.\n",
    "- These confirm that; negative values indicate the order has been cancelled. \n",
    "<br>So lets find out how many orders were cancelled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cancelled'].value_counts()  # 1 --> cancelled based on our tortured dataset (8872)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('Online Retail.csv',index_col=0)  # True --> cancelled based on our original dataset (9288)\n",
    "df_original[\"InvoiceNo\"].str.startswith('C').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9288 / len(df_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8872/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nunique(): returns number of unique elements in the specified column. It excludes NaN values by default.\n",
    "# Don’t include NaN in the count.\n",
    "df[df.cancelled == 1]['customerid'].nunique() / df['customerid'].nunique()  # ratio obtained based on our tortured dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ratio obtained based on our original dataset \n",
    "df_original[df_original[\"InvoiceNo\"].str.startswith('C')]['CustomerID'].nunique() / df_original['CustomerID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OI-xuxxiqa5"
   },
   "outputs": [],
   "source": [
    "# We will not use cancelled column again\n",
    "df = df.drop(['cancelled'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrp8logRQGWA"
   },
   "source": [
    "- 9288 or 1.7% (Original dataset) / 8872 or 2% (Tortured dataset) orders were cancelled.  \n",
    "- 36% --> cancelled and at the same time [\"customerid\"].nunique() ratio \n",
    "- Looking deeper into why these orders were cancelled may prevent future cancellations. Now let's find out what a negative UnitPrice means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXVVls6sQGVQ"
   },
   "source": [
    "### iii. Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yExVxnQsiqa6"
   },
   "outputs": [],
   "source": [
    "missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQtZK5paQGVf"
   },
   "source": [
    "- Let's assume that the orders which do NOT have customer ID's were not made by the customers already in the dataset because the customers who in fact made some purchases already have ID's.\n",
    "- So we don't want to assign these orders to those customers because this would alter the insights we draw from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjvtekM3iqa6"
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['customerid'])  # we already did at the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llu-bMTAiqa6"
   },
   "source": [
    "### iv. Clean the Data from the Noise and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-TgtE4Ziqa6"
   },
   "outputs": [],
   "source": [
    "# So, with this operation, lines containing canceled orders are dropped.\n",
    "df = df[(df.quantity > 0) & (df.unitprice > 0)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25MkNjZqQGWC"
   },
   "source": [
    "### v. Explore the Orders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OAkPoUjiqa7"
   },
   "source": [
    "1. Find the unique number of InvoiceNo  per customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1A76M7Jiqa7"
   },
   "outputs": [],
   "source": [
    "df.groupby('customerid')['invoiceno'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "di03OKjzQGWE"
   },
   "source": [
    "2. What's the average number of unqiue items per order or per customer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *1.Way (Detailed)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code gives us very good insight but very detailed information. \n",
    "# In the next lines we get a more general and understandable output. \n",
    "# There is no difference between including the 'invoiceno' column or not. It's just a matter of details.\n",
    "df.groupby(['customerid', \"invoiceno\", 'stockcode', 'description'])['quantity'].mean() # Length: 387843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['customerid', \"invoiceno\", 'stockcode'])['quantity'].mean()  # Length: 387841 --> So 2 different descriptions written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *2.Way(More General)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4LxkeIIiqa7"
   },
   "outputs": [],
   "source": [
    "df.groupby(['customerid', 'stockcode'])['quantity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['invoiceno', 'stockcode'])['quantity'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUKzA73dQGWH"
   },
   "source": [
    "3. Let's see how this compares to the number of unique products per customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *1.Way (Detailed)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code gives us very good insight but very detailed information. \n",
    "# In the next line we get a more general and understandable output. \n",
    "# There is no difference between including the 'invoiceno' column or not. It's just a matter of details.\n",
    "df.groupby(['customerid', 'invoiceno', 'stockcode'])['quantity'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *2.Way(More General)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8uzjYyKiqa7"
   },
   "outputs": [],
   "source": [
    "df.groupby(['customerid', 'stockcode'])['quantity'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Ozp-U5QGWK"
   },
   "source": [
    "### vi. Explore Customers by Country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SP6M3isLiqa8"
   },
   "source": [
    "1. What's the total revenue per country?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Revenue means total price.*\n",
    "- *Total Price = Quantity X Unit Price*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VISxZ3Ixiqa8"
   },
   "outputs": [],
   "source": [
    "# The calculation of total price or aka. revenue. \n",
    "df['total_price'] = df['quantity'] * df['unitprice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It was a very confusing outcome. Let's sort it out and get a nicer insight in the next line.\n",
    "df.groupby('country')[[\"total_price\"]].sum().sort_values(by=\"total_price\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df.groupby('country')[\"total_price\"].sum().sort_values(ascending=False),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk64qtEliqa8"
   },
   "source": [
    "2. Visualize number of customer per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's check how many unique customerid I have\n",
    "df[\"customerid\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmm! But when I check again while grouping by countries, a different result comes out.\n",
    "# To understand what's that difference, read the next line.\n",
    "df.groupby('country')[\"customerid\"].nunique().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('customerid')['country'].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4338 unique \"customerid\". But when we are grouping them using with contry column; Our unique number of customers is increasing. <br>It means some \"customerid\" (totally 8) are included in multiple country names. For ex: *'12431.0'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atuYU8W2iqa8"
   },
   "outputs": [],
   "source": [
    "# MATPLOTLIB\n",
    "plt.figure(figsize=(16,9))\n",
    "df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEABORN\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.barplot(y = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).index, \n",
    "            x = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTLY BAR PLOT\n",
    "import plotly.express as px\n",
    "plt.figure(figsize = (16,9))\n",
    "fig = px.bar(df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False),\n",
    "             x = ['customerid'], \n",
    "             y = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).index,\n",
    "             title=\"Number of Customers Per Country\")\n",
    "fig.update_yaxes(categoryorder='total ascending')\n",
    "fig.update_layout(xaxis_title=\"NUMBER OF CUSTOMER\",yaxis_title=\"COUNTRIES\", autosize=False, width=1400, height=700, title_font_color=\"red\",)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTLY TREEMAP\n",
    "fig = px.treemap(df.groupby('country')[[\"customerid\"]].nunique().sort_values(by=\"customerid\", ascending=False),\n",
    "                 path=[df.groupby('country')[[\"customerid\"]].nunique().sort_values(by=\"customerid\", ascending=False).index], \n",
    "                 values='customerid', \n",
    "                 width=1000, \n",
    "                 height=600)\n",
    "fig.update_layout(title_text='Number of Customers Per Country',\n",
    "                  title_x = 0.5, title_font = dict(size=20)\n",
    ")\n",
    "fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's see what our total unique number of customers graph would look like if we didn't include United Kingdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATPLOTLIB\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.set_style(\"whitegrid\")\n",
    "df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).iloc[1:].plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEABORN\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.barplot(y = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).iloc[1:].index, \n",
    "            x = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).iloc[1:].values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTLY BAR PLOT\n",
    "import plotly.express as px\n",
    "plt.figure(figsize = (16,9))\n",
    "fig = px.bar(df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False),\n",
    "             x = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).iloc[1:], \n",
    "             y = df.groupby('country')[\"customerid\"].nunique().sort_values(ascending=False).iloc[1:].index,\n",
    "             title=\"Number of Customers Per Country Without UK\")\n",
    "fig.update_yaxes(categoryorder='total ascending')\n",
    "fig.update_layout(xaxis_title=\"NUMBER OF CUSTOMER\",yaxis_title=\"COUNTRIES\", autosize=False, width=1400, height=700, title_font_color=\"red\",)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTLY TREEMAP\n",
    "fig = px.treemap(df.groupby('country')[[\"customerid\"]].nunique().sort_values(by=\"customerid\", ascending=False).iloc[1:],\n",
    "                 path=[df.groupby('country')[[\"customerid\"]].nunique().sort_values(by=\"customerid\", ascending=False).iloc[1:].index], \n",
    "                 values='customerid', \n",
    "                 width=1000, \n",
    "                 height=600)\n",
    "fig.update_layout(title_text='Number of Customers Per Country Without UK',\n",
    "                  title_x = 0.5, title_font = dict(size=20)\n",
    ")\n",
    "fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These plots gives us the total number of unique customers for each country on a country basis.\n",
    "- In general, we can say the following at first sight: \n",
    "<br>*According to the dataset we have, United Kingdom has an overwhelming advantage compared to other countries in terms of total number of unique customers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TD66fT3iqa8"
   },
   "source": [
    "3. Visualize total cost per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5i0nVOggiqa8"
   },
   "outputs": [],
   "source": [
    "# We already created total price (aka. revenue) before. Let's use it!\n",
    "pd.DataFrame(df.groupby('country').total_price.sum().sort_values(ascending=False)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save this dataframe for future analysis.\n",
    "df_total_price = pd.DataFrame(df.groupby('country')[\"total_price\"].sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATPLOTLIB\n",
    "# Attention to up left corner of plot --> 1e6 means 10**6\n",
    "df_total_price.plot(kind=\"bar\", width=0.7, color='lightgreen', edgecolor='darkgreen', figsize=(16,9));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEABORN\n",
    "plt.figure(figsize=(16,9))\n",
    "# plt.xlim(145.920, 7285024.644) # To see and understand the_total price's range.\n",
    "sns.barplot(data = df_total_price, x = df_total_price[\"total_price\"], y = df_total_price.index);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTLY BAR PLOT\n",
    "import plotly.express as px\n",
    "plt.figure(figsize = (16,9))\n",
    "fig = px.bar(df_total_price, \n",
    "             x = \"total_price\", \n",
    "             y = df_total_price.index, \n",
    "             title=\"Total Prices Per Country\")\n",
    "fig.update_yaxes(categoryorder='total ascending')\n",
    "fig.update_layout(xaxis_title=\"TOTAL_PRICE\",yaxis_title=\"COUNTRIES\", autosize=False, width=1400, height=700, title_font_color=\"red\",)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTLY TREEMAP\n",
    "fig = px.treemap(df_total_price,\n",
    "                 path=[df_total_price.index], \n",
    "                 values='total_price', \n",
    "                 width=1000, \n",
    "                 height=600)\n",
    "fig.update_layout(title_text='Total Prices By Countries',\n",
    "                  title_x = 0.5, title_font = dict(size=20)\n",
    ")\n",
    "fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's see what our total price graph would look like if we didn't include United Kingdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATPLOTLIB\n",
    "# Attention to up left corner of plot --> 1e6 means 10**6\n",
    "df_total_price.iloc[1:].plot(kind=\"bar\", width=0.7, color='lightgreen', edgecolor='darkgreen', figsize=(16,9));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEABORN\n",
    "plt.figure(figsize = (16,9))\n",
    "sns.barplot(data = df_total_price.iloc[1:], x = df_total_price.iloc[1:][\"total_price\"], y = df_total_price.iloc[1:].index);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PLOTLY BAR PLOT\n",
    "import plotly.express as px\n",
    "plt.figure(figsize = (16,9))\n",
    "fig = px.bar(df_total_price.iloc[1:], x = \"total_price\", y = df_total_price.iloc[1:].index, title=\"Total Prices Per Country Without UK\")\n",
    "fig.update_yaxes(categoryorder='total ascending')\n",
    "fig.update_layout(xaxis_title=\"TOTAL_PRICE\",yaxis_title=\"COUNTRIES\", autosize=False, width=1400, height=700, title_font_color=\"red\",)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PLOTLY TREEMAP\n",
    "fig = px.treemap(df_total_price.iloc[1:],\n",
    "                 path=[df_total_price.iloc[1:].index], \n",
    "                 values='total_price', \n",
    "                 width=1000, \n",
    "                 height=600)\n",
    "fig.update_layout(title_text='Total Prices By Countries Without UK',\n",
    "                  title_x = 0.5, title_font = dict(size=20)\n",
    ")\n",
    "fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwfJuBVCQGWR"
   },
   "source": [
    "- Result: \n",
    "<br>The UK is not only the country with the most sales revenue, but also the country with the most customers.\n",
    "<br>The majority of this dataset includes orders from the UK.\n",
    "<br>So we can further explore the UK market by learning which products customers buy together and other buying behaviors to improve our sales and targeting strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A25nnqIQGWR"
   },
   "source": [
    "### vii. Explore the UK Market\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtWchB1Ziqa9"
   },
   "source": [
    "1. Create df_uk DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsdH3cuZiqa9"
   },
   "outputs": [],
   "source": [
    "df_uk = df[df[\"country\"] == 'United Kingdom']\n",
    "df_uk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is5kus2bQGWT"
   },
   "source": [
    "2. What are the most popular products that are bought in the UK?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCN56amLiqa9"
   },
   "source": [
    "- Let's take a look at our columns related to the product, from \"Determines\" section. Here the story of our dataset.\n",
    "    - **InvoiceNo**: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
    "    - **StockCode**: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
    "    - **Description**: Product (item) name. Nominal.\n",
    "    - **Quantity**: The quantities of each product (item) per transaction. Numeric.\n",
    "    - **InvoiceDate**: Invoice Date and time. Numeric, the day and time when each transaction was generated.\n",
    "    - **UnitPrice**: Unit price. Numeric, Product price per unit in sterling.\n",
    "    - **CustomerID**: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
    "    - **Country**: Country name. Nominal, the name of the country where each customer resides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's see how much of each product UK has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk[\"stockcode\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 most owned products.\n",
    "df_uk[\"stockcode\"].value_counts().head(15).plot(kind=\"bar\", width=0.5, color='pink', edgecolor='purple', figsize=(16,9))\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHbqJD7bQGWU"
   },
   "source": [
    "- We will continue analyzing the UK transactions with customer segmentation.\n",
    "- **Let's start the RFM analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAtzWvugQGWV"
   },
   "source": [
    "# 2. RFM Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5uUAUQpQGWV"
   },
   "source": [
    "In the age of the internet and e-commerce, companies that do not expand their businesses online or utilize digital tools to reach their customers will run into issues like scalability and a lack of digital precsence. An important marketing strategy e-commerce businesses use for analyzing and predicting customer value is customer segmentation. Customer data is used to sort customers into group based on their behaviors and preferences.\n",
    "\n",
    "**[RFM](https://www.putler.com/rfm-analysis/) (Recency, Frequency, Monetary) Analysis** is a customer segmentation technique for analyzing customer value based on past buying behavior. RFM analysis was first used by the direct mail industry more than four decades ago, yet it is still an effective way to optimize your marketing.\n",
    "<br>\n",
    "<br>\n",
    "Our goal in this Notebook is to cluster the customers in our data set to:\n",
    " - Recognize who are our most valuable customers\n",
    " - Increase revenue\n",
    " - Increase customer retention\n",
    " - Learn more about the trends and behaviors of our customers\n",
    " - Define customers that are at risk\n",
    "\n",
    "We will start with **RFM Analysis** and then compliment our findings with predictive analysis using **K-Means Clustering Algorithms.**\n",
    "\n",
    "- RECENCY (R): Time since last purchase\n",
    "- FREQUENCY (F): Total number of purchases\n",
    "- MONETARY VALUE (M): Total monetary value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Benefits of RFM Analysis\n",
    "\n",
    "- Increased customer retention\n",
    "- Increased response rate\n",
    "- Increased conversion rate\n",
    "- Increased revenue\n",
    "\n",
    "RFM Analysis answers the following questions:\n",
    " - Who are our best customers?\n",
    " - Who has the potential to be converted into more profitable customers?\n",
    " - Which customers do we need to retain?\n",
    " - Which group of customers is most likely to respond to our marketing campaign?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can use \"invoicedate\" column to find when the products were last received. \n",
    "- Let's do it both for UK and for the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rYM4MJsQGWW"
   },
   "source": [
    "### ii. Review df_uk DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvfCcPLgQGWa"
   },
   "source": [
    "### iii. Recency: Days since last purchase\n",
    "To calculate the recency values, follow these steps in order:\n",
    "\n",
    "1. To calculate recency, we need to choose a date as a point of reference to evaluate how many days ago was the customer's last purchase.\n",
    "2. Create a new column called Date which contains the invoice date without the timestamp\n",
    "3. Group by CustomerID and check the last date of purchase\n",
    "4. Calculate the days since last purchase\n",
    "5. Drop Last_Purchase_Date since we don't need it anymore\n",
    "6. Plot RFM distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7Gzn6r6QGWb"
   },
   "source": [
    "1. Choose a date as a point of reference to evaluate how many days ago was the customer's last purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now find the last and first selling date of my whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_invoice = max(df_uk['invoicedate'])\n",
    "last_invoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_invoice = min(df_uk['invoicedate'])\n",
    "first_invoice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ge34PCT0iqa-"
   },
   "source": [
    "2. Create a new column called Date which contains the invoice date without the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk[\"invoicedate\"].dtype.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to change it's dtype as datetime\n",
    "df_uk['invoicedate'] = pd.to_datetime(df_uk['invoicedate'])\n",
    "df_uk[\"invoicedate\"].dtype.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same for the whole dataset, but it is not necessary.\n",
    "df['invoicedate'] = pd.to_datetime(df['invoicedate'])\n",
    "df[\"invoicedate\"].dtype.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create a date column with only dates, need to extract time part (hour-min-sec etc). --invoice date without the timestamp--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk[\"date\"] = df_uk['invoicedate'].dt.date\n",
    "df_uk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not necessary\n",
    "df[\"date\"] = df['invoicedate'].dt.date\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KJzP5Ldiqa_"
   },
   "source": [
    "3. Group by CustomerID and check the last date of purchase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we will group dates by customers.\n",
    "<br>But we can not use sum, unique, nunique etc directly. We can use transform and will get max or min or sum or count etc..\n",
    "<br>https://pbpython.com/pandas_transform.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TRANSFORM: provides to combine the output data back with the original dataframe.** Without using transform we can't get output's length same like dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.groupby(\"customerid\").date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.groupby(\"customerid\").date.transform(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same length both of my last date and df_uk dataset.\n",
    "# Because  I have unique customer ids here and I used transform\n",
    "len(df_uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add this into dataset\n",
    "df_uk[\"last_purchased_date\"] = df_uk.groupby(\"customerid\").date.transform(max)\n",
    "df_uk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Zxs1mzPiqa_"
   },
   "source": [
    "4. Calculate the days since last purchase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- That's the recency : Days since last purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_invoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_invoice = pd.to_datetime(last_invoice).date()\n",
    "last_invoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.groupby('customerid')['last_purchased_date'].apply(lambda x: last_invoice - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk[\"recency\"] = df_uk.groupby('customerid')['last_purchased_date'].apply(lambda x: last_invoice - x)\n",
    "df_uk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAuMSkfsiqa_"
   },
   "source": [
    "5. Drop Last_Purchase_Date since we don't need it anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk = df_uk.drop('last_purchased_date',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bNmKqHNiqa_"
   },
   "outputs": [],
   "source": [
    "df_uk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IB9a0AL9iqa_"
   },
   "source": [
    "6. Plot RFM distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.recency.dtype.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk['recency'] = pd.to_numeric(df_uk['recency'].dt.days, downcast='integer')\n",
    "df_uk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.recency.dtype.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2 differences between hist plot and distplot:\n",
    "<br>1. Dist plot adds the kde curve by default and brings the density on the vertical axis.\n",
    "<br>2. If you specified kde = True on the hist plot, it returns the kde curve and returns the count value on the vertical axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8e-QiwRiqa_"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "sns.distplot(df_uk['recency'], bins=35);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "sns.histplot(df_uk['recency'], kde = True, bins=35);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAjKZD0KQGWg"
   },
   "source": [
    "### iv. Frequency: Number of purchases\n",
    "\n",
    "To calculate how many times a customer purchased something, we need to count how many invoices each customer has. To calculate the frequency values, follow these steps in order:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDoNslseiqbA"
   },
   "source": [
    "1. Make a copy of df_uk and drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gk2gokFiqbA"
   },
   "outputs": [],
   "source": [
    "df_uk.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop anyway\n",
    "df_uk = df_uk.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KrnuXrLiqbA"
   },
   "source": [
    "2. Calculate the frequency of purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LTM_cxpiqbA"
   },
   "source": [
    "- When we look at the definition of invoiceno, we see:\n",
    "<br>*Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.groupby('customerid')[\"invoiceno\"].count().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact length is gotten only by using transform\n",
    "df_uk.groupby('customerid')[\"invoiceno\"].transform('count').sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk['frequency'] = df_uk.groupby('customerid')[\"invoiceno\"].transform('count')\n",
    "df_uk.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9NNBCNgiqbA"
   },
   "source": [
    "3. Plot RFM distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUtZAHu1iqbA"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "sns.distplot(df_uk['frequency'], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "sns.histplot(df_uk['frequency'], bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUY3gKjQQGWh"
   },
   "source": [
    "### v. Monetary: Total amount of money spent\n",
    "\n",
    "The monetary value is calculated by adding together the cost of the customers' purchases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_4_QLWtiqbA"
   },
   "source": [
    "1. Calculate sum total cost by customers and named \"Monetary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bilKBqvIiqbB"
   },
   "outputs": [],
   "source": [
    "df_uk.groupby('customerid')[\"total_price\"].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk[\"monetary\"] = df_uk.groupby('customerid')[\"total_price\"].transform('sum')\n",
    "df_uk.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYsaCPRDiqbB"
   },
   "source": [
    "2. Plot RFM distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sd41fD67iqbB"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "sns.distplot(df_uk['monetary'], bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeaecPkSQGWj"
   },
   "source": [
    "### vi. Create RFM Table\n",
    "Merge the recency, frequency and motetary dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M88KNSbyiqbB"
   },
   "outputs": [],
   "source": [
    "df_rfm_table = df_uk[['customerid', 'recency', 'frequency', 'monetary']]\n",
    "df_rfm_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULWwsxCkQGWl"
   },
   "source": [
    "# 3. Customer Segmentation with RFM Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZrxUBX4iqbB"
   },
   "source": [
    "Businesses have this ever-lasting urge to understand their customers. The better you understand the customer, the better you serve them, and the higher the financial gain you receive from that customer. Since the dawn of trade, this process of understanding customers for a strategic gain has been there practiced and this task is known majorly as [Customer Segmentation](https://clevertap.com/blog/rfm-analysis/).\n",
    "Well as the name suggests, Customer Segmentation could segment customers according to their precise needs. Some of the common ways of segmenting customers are based on their Recency-Frequency-Monatory values, their demographics like gender, region, country, etc, and some of their business-crafted scores. You will use Recency-Frequency-Monatory values for this case.\n",
    "\n",
    "In this section, you will create an RFM Segmentation Table where you segment your customers by using the RFM table. For example, you can label the best customer as \"Big Spenders\" and the lost customer as \"Lost Customer\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Source : https://clevertap.com/blog/rfm-analysis/*<br>\n",
    "RFM stands for Recency, Frequency, and Monetary value, each corresponding to some key customer trait. These RFM metrics are important indicators of a customer’s behavior because frequency and monetary value affects a customer’s lifetime value, and recency affects retention, a measure of engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anOsOGpfQGWl"
   },
   "source": [
    "## Calculate RFM Scoring\n",
    "\n",
    "The simplest way to create customer segments from an RFM model is by using **Quartiles**. We will assign a score from 1 to 4 to each category (Recency, Frequency, and Monetary) with 4 being the highest/best value. The final RFM score is calculated by combining all RFM values. For Customer Segmentation, you will use the df_rfm data set resulting from the RFM analysis.\n",
    "<br>\n",
    "<br>\n",
    "**Note**: Data can be assigned into more groups for better granularity, but we will use 4 in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiwXSsP7iqbB"
   },
   "source": [
    "1. Divide the df_rfm into quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFQJGPYHiqbC"
   },
   "outputs": [],
   "source": [
    "df_rfm_table = df_rfm_table.set_index('customerid')\n",
    "df_rfm_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table.reset_index().duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table.drop_duplicates(inplace=True)\n",
    "df_rfm_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnkzCAf9QGWo"
   },
   "source": [
    "### i. Creating the RFM Segmentation Table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLIB-z-_iqbC"
   },
   "source": [
    "1. Create two functions, one for Recency and one for Frequency and Monetary. For Recency, customers in the first quarter should be scored as 4, this represents the highest Recency value. Conversely, for Frequency and Monetary, customers in the last quarter should be scored as 4, representing the highest Frequency and Monetary values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- help(np.quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLDK_XeLiqbC"
   },
   "source": [
    "2. Score customers from 1 to 4 by applying the functions you have created. Also create separate score column for each value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Recency*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table[\"recency\"].quantile(q = [.25,.5,.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXnW03R8iqbC"
   },
   "outputs": [],
   "source": [
    "def recency_scoring(data):\n",
    "    if data[\"recency\"] <= 17.000:\n",
    "        return 4\n",
    "    elif data[\"recency\"] <= 50.000:\n",
    "        return 3\n",
    "    elif data[\"recency\"] <= 142.000:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table['recency_quantile'] = df_rfm_table.apply(recency_scoring, axis =1)\n",
    "df_rfm_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Frequency*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table[\"frequency\"].quantile(q = [.25,.5,.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_scoring(data):\n",
    "    if data.frequency <= 17.000:\n",
    "        return 1\n",
    "    elif data.frequency <= 40.000:\n",
    "        return 2\n",
    "    elif data.frequency <= 98.000:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table['frequency_quantile'] = df_rfm_table.apply(frequency_scoring, axis =1)\n",
    "df_rfm_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Monetary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table[\"monetary\"].quantile(q = [.25,.5,.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monetary_scoring(data):\n",
    "    if data.monetary <= 298.185:\n",
    "        return 1\n",
    "    elif data.monetary <= 644.975:\n",
    "        return 2\n",
    "    elif data.monetary <= 1571.285:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table['monetary_quantile'] = df_rfm_table.apply(monetary_scoring, axis =1)\n",
    "df_rfm_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JskteCFdQGWq"
   },
   "source": [
    "3. Now that scored each customer, you'll combine the scores for segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYfoHF6QiqbC"
   },
   "outputs": [],
   "source": [
    "def rfm_scoring(data):\n",
    "    return str(int(data['recency_quantile'])) + str(int(data['frequency_quantile'])) + str(int(data['monetary_quantile']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table['rfm_score'] = df_rfm_table.apply(rfm_scoring, axis=1)\n",
    "df_rfm_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table[\"rfm_score\"].dtype.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table[\"rfm_score\"].dtype.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWwWeyjPiqbC"
   },
   "source": [
    "4. Define rfm_level function that tags customers by using RFM_Scrores and Create a new variable RFM_Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxXk7jFPiqbD"
   },
   "outputs": [],
   "source": [
    "df_rfm_table['rfm_level'] = df_rfm_table['recency_quantile'] + df_rfm_table['frequency_quantile'] + df_rfm_table['monetary_quantile']\n",
    "df_rfm_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq36PiX3iqbD"
   },
   "source": [
    "5. Calculate average values for each RFM_Level, and return a size of each segment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many of which \"rfm_score\" here are.\n",
    "df_rfm_table['rfm_score'].value_counts().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first way to create a \"segments\" column.\n",
    "    - Our minimum rfm_level score is 3 and maximum 12.\n",
    "    - We can roughly classify these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segments1(data):\n",
    "    if data['rfm_level'] >= 10 :\n",
    "        return 'Gold'\n",
    "    elif (data['rfm_level'] >= 6) and (data['rfm_level'] < 10 ):\n",
    "        return 'Sliver'\n",
    "    else:  \n",
    "        return 'Bronze'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table ['segments1'] = df_rfm_table.apply(segments1,axis=1)\n",
    "df_rfm_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The second way to create a \"segments\" column.\n",
    "    - Will be very detailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments2 = {'Customer Segment':['Champions','Loyal Customers','Potential Loyalist', 'Recent Customers', 'Customers Needing Attention', 'Still Got Hope', 'Need to Get Them Back','Lost', 'Give it a Try'],\\\n",
    "            'RFM':['(3|4)-(3|4)-(3|4)', '(2|3|4)-(3|4)-(1|2|3|4)', '(3|4)-(2|3)-(1|2|3|4)', '(4)-(1)-(1|2|3|4)', '(2|3)-(2|3)-(2|3)', '(2|3)-(1|2)-(1|2|3|4)', '(1|2)-(3|4)-(2|3|4)', '(1|2)-(1|2)-(1|2)','(1|2)-(1|2|3)-(1|2|3|4)']}\n",
    "pd.DataFrame(segments2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizer(rfm):\n",
    "    if (rfm[0] in ['3', '4']) & (rfm[1] in ['3', '4']) & (rfm[2] in ['3', '4']):\n",
    "        rfm = 'Champions'\n",
    "        \n",
    "    elif (rfm[0] in ['2', '3', '4']) & (rfm[1] in ['3', '4']) & (rfm[2] in ['1', '2', '3', '4']):\n",
    "        rfm = 'Loyal Customers'\n",
    "        \n",
    "    elif (rfm[0] in ['3', '4']) & (rfm[1] in ['2', '3']) & (rfm[2] in ['1', '2', '3', '4']):\n",
    "        rfm = 'Potential Loyalist'\n",
    "    \n",
    "    elif (rfm[0] in ['4']) & (rfm[1] in ['1']) & (rfm[2] in ['1', '2', '3', '4']):\n",
    "        rfm = 'Recent Customers'\n",
    "    \n",
    "    elif (rfm[0] in ['2', '3']) & (rfm[1] in ['2', '3']) & (rfm[2] in ['2', '3']):\n",
    "        rfm = 'Customers Needing Attention'\n",
    "    \n",
    "    elif (rfm[0] in ['2', '3']) & (rfm[1] in ['1', '2']) & (rfm[2] in ['1', '2', '3', '4']):\n",
    "        rfm = 'Still Got Hope'\n",
    "    \n",
    "    elif (rfm[0] in ['1', '2']) & (rfm[1] in ['3', '4']) & (rfm[2] in ['2', '3', '4']):\n",
    "        rfm = 'Need to Get Them Back'\n",
    "                \n",
    "    elif (rfm[0] in ['1', '2']) & (rfm[1] in ['1', '2']) & (rfm[2] in ['1', '2']):\n",
    "        rfm = 'Lost'\n",
    "    \n",
    "    elif (rfm[0] in ['1', '2']) & (rfm[1] in ['1', '2', '3']) & (rfm[2] in ['1', '2', '3', '4']):\n",
    "        rfm = 'Give it a Try'\n",
    "    \n",
    "    return rfm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table['segments2'] = df_rfm_table[\"rfm_score\"].apply(categorizer)\n",
    "df_rfm_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table[\"segments2\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuZ5Olo4iqbD"
   },
   "source": [
    "## Plot RFM Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STfELckwiqbD"
   },
   "source": [
    "1. Create your plot and resize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Segments1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oef37q3diqbD"
   },
   "outputs": [],
   "source": [
    "df_plot1 = pd.DataFrame(df_rfm_table[\"segments1\"].value_counts(dropna=False).sort_values(ascending=False)).reset_index().rename(columns={'index':'Segments', 'segments1':'Customers'})\n",
    "df_plot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATPLOTLIB\n",
    "y = df_plot1.Customers.values\n",
    "x = df_plot1.Segments.values\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (16,9), dpi=72)\n",
    "ax.bar(x,y,width=0.4)\n",
    "\n",
    "plt.title(\"RFM PLOT WITH SEGMENTS1\")\n",
    "plt.xlabel(\"SEGMENTS1\");\n",
    "plt.ylabel(\"CUSTOMERS\");\n",
    "\n",
    "for index,value in enumerate(y):\n",
    "    plt.text(x=index , y =value , s=str(value) , ha=\"center\", va=\"bottom\", color = 'gray', fontweight = 'bold', fontdict=dict(fontsize=20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEABORN\n",
    "\n",
    "# Show Values on Seaborn Barplot\n",
    "def show_values(axs, orient=\"v\", space=.01):\n",
    "    def _single(ax):\n",
    "        if orient == \"v\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() / 2\n",
    "                _y = p.get_y() + p.get_height() + (p.get_height()*0.01)\n",
    "                value = '{:.1f}'.format(p.get_height())\n",
    "                ax.text(_x, _y, value, ha=\"center\") \n",
    "        elif orient == \"h\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() + float(space)\n",
    "                _y = p.get_y() + p.get_height() - (p.get_height()*0.5)\n",
    "                value = '{:.1f}'.format(p.get_width())\n",
    "                ax.text(_x, _y, value, ha=\"left\")\n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _single(ax)\n",
    "    else:\n",
    "        _single(axs)\n",
    "        \n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.title(\"RFM PLOT WITH SEGMENTS1\")\n",
    "p = sns.barplot(data = df_plot1, x = 'Segments', y = 'Customers', palette = 'viridis')\n",
    "p.set(xlabel='SEGMENTS1', ylabel='CUSTOMERS')\n",
    "show_values(p);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PLOTLY\n",
    "\n",
    "fig = px.bar(df_plot1, x = \"Segments\", y = \"Customers\", width=950, height=600)\n",
    "fig.update_yaxes(categoryorder='total ascending')\n",
    "fig.update_layout(title=\"RFM PLOT WITH SEGMENTS1\", xaxis_title=\"SEGMENTS1\", yaxis_title=\"CUSTOMERS\", legend_title=\"Segments1\", title_font_color=\"red\", title_x=0.5,\n",
    "                  font=dict(family=\"Courier New, monospace\", size=20, color=\"RebeccaPurple\")\n",
    "                 )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTLY TREEMAP\n",
    "\n",
    "fig = px.treemap(df_plot1,\n",
    "                 path=[df_plot1.Segments], \n",
    "                 values='Customers', \n",
    "                 width=1000, \n",
    "                 height=600)\n",
    "fig.update_layout(title=\"RFM PLOT WITH SEGMENTS1\",\n",
    "                  title_x = 0.5, title_font = dict(size=20),\n",
    "                 )\n",
    "fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Segments2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot2 = pd.DataFrame(df_rfm_table[\"segments2\"].value_counts(dropna=False).sort_values(ascending=False)).reset_index().rename(columns={'index':'Segments', 'segments2':'Customers'})\n",
    "df_plot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATPLOTLIB\n",
    "y = df_plot2.Customers.values\n",
    "x = df_plot2.Segments.values\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (16,9), dpi=72)\n",
    "ax.bar(x,y,width=0.4)\n",
    "\n",
    "plt.title(\"RFM PLOT WITH SEGMENTS2\")\n",
    "plt.xlabel(\"SEGMENTS2\");\n",
    "plt.ylabel(\"CUSTOMERS\");\n",
    "plt.xticks(rotation = 45)\n",
    "\n",
    "for index,value in enumerate(y):\n",
    "    plt.text(x=index , y =value , s=str(value) , ha=\"center\", va=\"bottom\", color = 'gray', fontweight = 'bold', fontdict=dict(fontsize=20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEABORN\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.title(\"RFM PLOT WITH SEGMENTS2\")\n",
    "ax = sns.barplot(data = df_plot2, x = 'Segments', y = 'Customers', palette = 'viridis')\n",
    "plt.xticks(rotation = 45)\n",
    "ax.set(xlabel='SEGMENTS2', ylabel='CUSTOMERS')\n",
    "show_values(ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTLY\n",
    "\n",
    "fig = px.bar(df_plot2, x = \"Customers\", y = \"Segments\", width=950, height=600)\n",
    "fig.update_yaxes(categoryorder='total ascending')\n",
    "fig.update_layout(title=\"RFM PLOT WITH SEGMENTS2\", yaxis_title=\"SEGMENTS2\", xaxis_title=\"CUSTOMERS\", legend_title=\"Segments2\", title_font_color=\"red\", title_x=0.8,\n",
    "                  font=dict(family=\"Courier New, monospace\", size=20, color=\"RebeccaPurple\")\n",
    "                 )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTLY TREEMAP\n",
    "\n",
    "fig = px.treemap(df_plot2,\n",
    "                 path=[df_plot2.Segments], \n",
    "                 values='Customers', \n",
    "                 width=1000, \n",
    "                 height=600)\n",
    "fig.update_layout(title=\"RFM PLOT WITH SEGMENTS2\",\n",
    "                  title_x = 0.5, title_font = dict(size=20),\n",
    "                 )\n",
    "fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_plot2['Customers'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_plot2[\"Segments\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATPLOTLIB SQUARIFY\n",
    "\n",
    "import squarify\n",
    "\n",
    "fig = plt.gcf()\n",
    "ax = fig.add_subplot()\n",
    "fig.set_size_inches(16, 9)\n",
    "squarify.plot(sizes=list(df_plot2['Customers'].values), \n",
    "              label=list(df_plot2['Segments'].values), alpha=0.8,\n",
    "              text_kwargs={'fontsize':12})\n",
    "plt.title(\"RFM Segments\",fontsize=22,fontweight=\"bold\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhOe2bb6QGWu"
   },
   "source": [
    "Using customer segmentation categories found [here](http://www.blastam.com/blog/rfm-analysis-boosts-sales) we can formulate different marketing strategies and approaches for customer engagement for each type of customer.\n",
    "\n",
    "Note: The author in the article scores 1 as the highest and 4 as the lowest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Te_6gUR5iqbD"
   },
   "source": [
    "2. How many customers do we have in each segment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gs4rP-0viqbD"
   },
   "outputs": [],
   "source": [
    "df_rfm_table[\"segments1\"].value_counts(), df_rfm_table[\"segments2\"].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RwemvLyQGWv"
   },
   "source": [
    "# 4. Applying K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6WZ0PnZQGWv"
   },
   "source": [
    "Now that we have our customers segmented into 6 different categories, we can gain further insight into customer behavior by using predictive models in conjuction with out RFM model.\n",
    "Possible algorithms include **Logistic Regression**, **K-means Clustering**, and **K-nearest Neighbor**. We will go with [K-Means](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1) since we already have our distinct groups determined. K-means has also been widely used for market segmentation and has the advantage of being simple to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrWIRLkMiqbE"
   },
   "source": [
    "## Data Pre-Processing and Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLoRGR6NiqbE"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_table.groupby('rfm_level').agg(\n",
    "                                 {'recency': ['mean','min','max','count'],\n",
    "                                  'frequency': ['mean','min','max','count'],\n",
    "                                  'monetary': ['mean','min','max','count']}\n",
    "                                ).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6SGV0xoQGWw"
   },
   "source": [
    "### i. Define and Plot Feature Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpMAiWNBiqbE"
   },
   "source": [
    "Create Heatmap and evaluate the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6Cv8_EqiqbE"
   },
   "outputs": [],
   "source": [
    "df_rfm_table.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "sns.heatmap(data = df_rfm_table.corr(), annot=True, cmap = 'viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WhL5MBEQGWy"
   },
   "source": [
    "### ii. Visualize Feature Distributions\n",
    "\n",
    "To get a better understanding of the dataset, you can costruct a scatter matrix of each of the three features in the RFM data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"*segments1*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHE0Vb0KiqbE"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.pairplot(df_rfm_table[['recency', 'frequency', 'monetary','segments1']]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.pairplot(df_rfm_table[['recency', 'frequency', 'monetary','segments1']],hue='segments1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"*segments2*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.pairplot(df_rfm_table[['recency', 'frequency', 'monetary','segments2']]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.pairplot(df_rfm_table[['recency', 'frequency', 'monetary','segments2']], hue = \"segments2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of RFM values\n",
    "f,ax = plt.subplots(figsize=(10, 12))\n",
    "plt.subplot(3, 1, 1); sns.distplot(df_rfm_table[\"recency\"], label = 'Recency')\n",
    "plt.subplot(3, 1, 2); sns.distplot(df_rfm_table[\"frequency\"], label = 'Frequency')\n",
    "plt.subplot(3, 1, 3); sns.distplot(df_rfm_table[\"monetary\"], label = 'Monetary')\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2RsjzjbQGWz"
   },
   "source": [
    "### iii. Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXsrPpegiqbF"
   },
   "source": [
    "1. You can use the logarithm method to normalize the values in a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- COMPARISON OF LOG() AND LOG1P()\n",
    "- For real-valued input, log1p is accurate also for x so small that 1 + x == 1 in floating-point accuracy.<br>\n",
    "The log() function computes the value of the natural logarithm of argument x.<br>\n",
    "The log1p() function computes the value of log(1+x) accurately even for tiny argument x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = {'log1p': df_rfm_table['recency'].apply(np.log1p).values,\n",
    "           'log'  : (df_rfm_table['recency'] + 0.1).apply(np.log).values}\n",
    "\n",
    "pd.DataFrame(compare, index = df_rfm_table.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-cBU8jxiqbF"
   },
   "outputs": [],
   "source": [
    "rfm_log = df_rfm_table[['recency', 'frequency', 'monetary']].apply(np.log1p).round(3)\n",
    "rfm_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC6dHnqKiqbF"
   },
   "source": [
    "2. Plot normalized data with scatter matrix or pairplot. Also evaluate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iB9jDUPriqbF"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.pairplot(data = rfm_log);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(10, 12))\n",
    "plt.subplot(3, 1, 1); sns.distplot(rfm_log.recency, label = 'Recency')\n",
    "plt.subplot(3, 1, 2); sns.distplot(rfm_log.frequency, label = 'Frequency')\n",
    "plt.subplot(3, 1, 3); sns.distplot(rfm_log.monetary, label = 'Monetary')\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the variables with StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(rfm_log)\n",
    "#Store it separately for clustering\n",
    "rfm_normalized= scaler.transform(rfm_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35c0aDixQGW4"
   },
   "source": [
    "## K-Means Implementation\n",
    "\n",
    "For k-means, you have to set k to the number of clusters you want, but figuring out how many clusters is not obvious from the beginning. We will try different cluster numbers and check their [silhouette coefficient](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html). The silhouette coefficient for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). \n",
    "<br>\n",
    "<br>\n",
    "**Note**: K-means is sensitive to initializations because they are critical to qualifty of optima found. Thus, we will use smart initialization called \"Elbow Method\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from pyclustertend import hopkins\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.metrics import silhouette_samples,silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JloMSEeriqbF"
   },
   "source": [
    "### i. Define the Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Hopkins statistic (introduced by Brian Hopkins and John Gordon Skellam) is a way of measuring the cluster tendency of a data set. It belongs to the family of sparse sampling tests. It acts as a statistical hypothesis test where the null hypothesis is that the data is generated by a Poisson point process and are thus uniformly randomly distributed. \n",
    "- **A value close to 1 tends to indicate the data is highly clustered, random data will tend to result in values around 0.5, and uniformly distributed data will tend to result in values close to 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hopkins(rfm_log,rfm_log.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our Hopkins value is too close to 0 which means we have a dataset which has a quite small tendency to clustering. We need to further analyse with silouhette scores whether our data has a tendency to clustering or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McFq6IWZt5hg"
   },
   "source": [
    "[The Elbow Method](https://en.wikipedia.org/wiki/Elbow_method_(clustering) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2202eo2riqbF"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(1,10))\n",
    "\n",
    "visualizer.fit(rfm_log)        # Fit the data to the visualizer\n",
    "visualizer.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First : Get the Best KMeans \n",
    "ks = range(1,10)\n",
    "inertias=[]\n",
    "for k in ks :\n",
    "    # Create a KMeans clusters\n",
    "    kc = KMeans(n_clusters=k,random_state=1)\n",
    "    kc.fit(rfm_log)\n",
    "    inertias.append(kc.inertia_)\n",
    "\n",
    "# Plot ks vs inertias\n",
    "f, ax = plt.subplots(figsize=(15, 8))\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('Number of clusters, k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xticks(ks)\n",
    "plt.style.use('ggplot')\n",
    "plt.title('What is the Best Number for KMeans ?')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACT_d0UpwUSC"
   },
   "source": [
    "[Silhouette Coefficient](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qS4TLbRniqbG"
   },
   "outputs": [],
   "source": [
    "for k in range(2,10):\n",
    "\n",
    "    model = KMeans(n_clusters=k)  #  random_state=10\n",
    "    cluster_labels = model.fit_predict(rfm_log)\n",
    "    \n",
    "    print(\"For n_clusters =\", k,\n",
    "          \"The average silhouette_score is :\", silhouette_score(rfm_log, cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOTHER WAY\n",
    "\n",
    "ssd =[]\n",
    "\n",
    "for k in range(2,10):\n",
    "    model = KMeans(n_clusters=k)\n",
    "    model.fit(rfm_log)\n",
    "    ssd.append(model.inertia_)\n",
    "    print(f'Silhouette Score for {k} clusters: {silhouette_score(rfm_log, model.labels_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Silhouette_score decreases as the level of detail increases, that is, as the number of clusters (n_clusters) increases.\n",
    "- From this, the following conclusions can be drawn: Our first finding regarding the clustering tendency is supported by the score that we obtained with the Hopkins method, which we found to be very close to zero.\n",
    "- For the clustering, we will choose the **n_clusters=3** that have an acceptable elbow score.<br>\n",
    "(The Yellowbrick Elbow method recommends 7 clusters, but the silhouette score is too low for 7 n_clouster.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6dW2MWZiqbG"
   },
   "source": [
    "### ii. Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXM5ksoPiqbG"
   },
   "source": [
    "Fit the K-Means Algorithm with the optimal number of clusters you decided and save the model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geMuViLniqbG"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 3).fit(rfm_log)\n",
    "labels = kmeans.labels_\n",
    "rfm_log['labels']=labels\n",
    "\n",
    "rfm_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqcSwNZTQGW7"
   },
   "source": [
    "### iii. Visualize the Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cfx5kzPriqbG"
   },
   "source": [
    "1. Create a scatter plot and select cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyrovJB0iqbH"
   },
   "outputs": [],
   "source": [
    "plt.scatter(rfm_log.iloc[:,0], rfm_log.iloc[:,1], c = labels, s = 50, cmap = \"viridis\")\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red',alpha=0.5, label = 'Centroids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey = True, figsize = (14,6)) # sharey=True and y-axis labels are used in common.\n",
    "ax1.set_title('Recency-Frequency')\n",
    "ax1.set_xlabel('Recency')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.scatter(rfm_log[\"recency\"], rfm_log[\"frequency\"], c = kmeans.labels_, cmap = \"viridis\")\n",
    "ax1.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s = 300, alpha = 1, label = 'Centroids')\n",
    "\n",
    "ax2.set_title(\"Frequency-Monetary\")\n",
    "ax2.set_xlabel('Frequency')\n",
    "ax2.set_ylabel('Monetary')\n",
    "ax2.scatter(rfm_log[\"frequency\"], rfm_log[\"monetary\"], c = kmeans.labels_, cmap =\"viridis\")\n",
    "ax2.scatter(kmeans.cluster_centers_[:,1], kmeans.cluster_centers_[:,2], s = 300, alpha = 1, label = 'Centroids');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4sHOvlniqbH"
   },
   "source": [
    "2. Visualize Cluster Id vs Recency, Cluster Id vs Frequency and Cluster Id vs Monetary using Box plot. Also evaluate the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_log.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzPl-LXViqbH"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.boxplot(rfm_log['labels'], rfm_log['recency'])\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.boxplot(rfm_log['labels'], rfm_log['frequency'])\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.boxplot(rfm_log['labels'], rfm_log['monetary'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRyku5qJiqbH"
   },
   "source": [
    "### iv. Assign the Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVrkisf9iqbH"
   },
   "outputs": [],
   "source": [
    "rfm_log[\"labels\"].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfm_log['decision'] = rfm_log['labels'].map({2:'Best_Customers',1:'Almost_Lost',0:'Lost_Customers'})\n",
    "rfm_log['decision'] = rfm_log['labels'].apply(lambda item: 'Best_Customers' if item == 2 else (\"Almost_Lost\" if item == 1 else \"Lost_Customers\"))\n",
    "rfm_log.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DiG6sxS4gWH"
   },
   "source": [
    "**Conclusion**\n",
    "\n",
    "- Cluster 2 : The first cluster belongs to the \"Best Customers\" segment which we saw earlier as they purchase recently (R=4), frequent buyers (F=4), and spent the most (M=4)\n",
    "\n",
    "- Cluster 1 : Second cluster can be interpreted as passer customers as their last purchase is long ago (R<=1),purchased very few (F>=2 & F < 4) and spent little (M>=4 & M < 4).Company has to come up with new strategies to make them permanent members. Low value customers\n",
    "- Cluster 0 : The third cluster is more related to the \"Almost Lost\" segment as they Haven’t purchased for some time(R=1), but used to purchase frequently and spent a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFaHgoLoQGXA"
   },
   "source": [
    "How we want to continue this analysis depends on how the business plans to use the results and the level of granularity the business stakeholders want to see in the clusters. We can also ask what range of customer behavior from high to low value customers are the stakeholders interested in exploring. From those answers, various methods of clustering can be used and applied on RFM variable or directly on the transaction data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysCkU1B-iqbI"
   },
   "source": [
    "**Annotation:**\n",
    "\n",
    "Limitations of K-means clustering:\n",
    "\n",
    "1. There is no assurance that it will lead to the global best solution.\n",
    "2. Can't deal with different shapes(not circular) and consider one point's probability of belonging to more than one cluster.\n",
    "\n",
    "These disadvantages of K-means show that for many datasets (especially low-dimensional datasets), it may not perform as well as you might hope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiPd_IbnQGVn"
   },
   "source": [
    "# 5. Create Cohort & Conduct Cohort Analysis\n",
    "[Cohort Analysis](https://medium.com/swlh/cohort-analysis-using-python-and-pandas-d2a60f4d0a4d) is specifically useful in analyzing user growth patterns for products. In terms of a product, a cohort can be a group of people with the same sign-up date, the same usage starts month/date, or the same traffic source.\n",
    "Cohort analysis is an analytics method by which these groups can be tracked over time for finding key insights. This analysis can further be used to do customer segmentation and track metrics like retention, churn, and lifetime value.\n",
    "\n",
    "For e-commerce organizations, cohort analysis is a unique opportunity to find out which clients are the most valuable to their business. by performing Cohort analysis you can get the following answers to the following questions:\n",
    "\n",
    "- How much effective was a marketing campaign held in a particular time period?\n",
    "- Did the strategy employ to improve the conversion rates of Customers worked?\n",
    "- Should I focus more on retention rather than acquiring new customers?\n",
    "- Are my customer nurturing strategies effective?\n",
    "- Which marketing channels bring me the best results?\n",
    "- Is there a seasonality pattern in Customer behavior?\n",
    "- Along with various performance measures/metrics for your organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhiYivPrQGVo"
   },
   "source": [
    "Since we will be performing Cohort Analysis based on transaction records of customers, the columns we will be dealing with mainly:\n",
    "- Invoice Data\n",
    "- CustomerID\n",
    "- Price\n",
    "- Quantity\n",
    "\n",
    "The following steps will performed to generate the Cohort Chart of Retention Rate:\n",
    "- Month Extraction from InvioceDate column\n",
    "- Assigning Cohort to Each Transaction\n",
    "- Assigning Cohort Index to each transaction\n",
    "- Calculating number of unique customers in each Group of (ChortDate,Index)\n",
    "- Creating Cohort Table for Retention Rate\n",
    "- Creating the Cohort Chart using the Cohort Table\n",
    "\n",
    "The Detailed information about each step is given below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eo0GB_osiqbI"
   },
   "source": [
    "## Future Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVwPNjpyQGVo"
   },
   "source": [
    "### i. Extract the Month of the Purchase\n",
    "First we will create a function, which takes any date and returns the formatted date with day value as 1st of the same month and Year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQKsM_9IQGVq"
   },
   "source": [
    "Now we will use the function created above to convert all the invoice dates into respective month date format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cohort = df\n",
    "df_cohort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DL17u0dniqbJ"
   },
   "outputs": [],
   "source": [
    "def first_of_month(data):\n",
    "    return dt.datetime(dt.datetime.strptime(str(data), '%Y-%m-%d %H:%M:%S').year, \n",
    "                       dt.datetime.strptime(str(data), '%Y-%m-%d %H:%M:%S').month, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['invoicemonth'] = df['invoicedate'].apply(first_of_month)\n",
    "df['cohortmonth'] = df.groupby('customerid')['invoicemonth'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month(x) : \n",
    "    return dt.datetime(x.year,x.month,1)\n",
    "df['InvoiceMonth'] = df['InvoiceDate'].apply(get_month)\n",
    "df['CohortMonth'] = df.groupby('CustomerID')['InvoiceMonth'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPE7kTz2QGVs"
   },
   "source": [
    "### ii. Calculating time offset in Months i.e. Cohort Index:\n",
    "Calculating time offset for each transaction will allows us to report the metrics for each cohort in a comparable fashion.\n",
    "First, you will create 4 variables that capture the integer value of years, months for Invoice and Cohort Date using the get_date_int() function which you'll create it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_3aYf4FiqbJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGm1eweDQGVu"
   },
   "source": [
    "You will use this function to extract the integer values for Invoice as well as Cohort Date in 3 seperate series for each of the two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wF_ViD_iqbJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9jYVljviqbJ"
   },
   "source": [
    "Use the variables created above to calcualte the difference in days and store them in cohort Index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVlAYCbEiqbJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-t76CXSQGVw"
   },
   "source": [
    "## Create 1st Cohort: User number & Retention Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKvUWci9iqbJ"
   },
   "source": [
    "### i. Pivot Cohort and Cohort Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-8HzlZWiqbK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63TIyBY6iqbK"
   },
   "source": [
    "### ii. Visualize analysis of cohort 1 using seaborn and matplotlib modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SY7mPvCAiqbK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yORYolvqQGV0"
   },
   "source": [
    "## Create the 2nd Cohort: Average Quantity Sold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu1hM3CFiqbK"
   },
   "source": [
    "### i. Pivot Cohort and Cohort Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQ8jlhPEiqbK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3PJHMS6iqbK"
   },
   "source": [
    "### ii. Visualize analysis of cohort 2 using seaborn and matplotlib modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vuHi3wPiqbK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUoG5yUIQGV3"
   },
   "source": [
    "## Create the 3rd Cohort: Average Sales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKNS-mO5iqbL"
   },
   "source": [
    "### i. Pivot Cohort and Cohort Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2s-zyWeiqbL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRGOpeUPiqbL"
   },
   "source": [
    "### ii. Visualize analysis of cohort 3 using seaborn and matplotlib modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYkkDncXiqbL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uD9lu1ExQGV5"
   },
   "source": [
    "For e-commerce organisations, cohort analysis is a unique opportunity to find out which clients are the most valuable to their business. by performing Cohort analysis you can get answers to following questions:\n",
    "\n",
    "- How much effective was a marketing campaign held in a particular time period?\n",
    "- Did the strategy employed to improve the conversion rates of Customers worked?\n",
    "- Should I focus more on retention rather than acquiring new customers?\n",
    "- Are my customer nurturing strategies effective?\n",
    "- Which marketing channels bring me the best results?\n",
    "- Is there a seasoanlity pattern in Customer behahiour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX_Y6S36iqbL"
   },
   "source": [
    "### ___\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lf4OsPmSQGXA"
   ],
   "name": "RFM - Customer Segmentation_Student_V3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
